{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Registration Documentation\n",
    "This is a notebook explaining the code behind dataset registration. In this notebook, we will document our data model and how we are building this database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Model\n",
    "\n",
    "We first designed a schema that we verified to be useful for lab members that use the http://m2g.io datasets. This schema is organized around subjects, and thus the data derivatives are all organized as children of subjects. We do however, organize subjects around each particular dataset.\n",
    "\n",
    "### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline/\n",
    "    dataset/\n",
    "        subject-level/\n",
    "            scan-level/\n",
    "                metadata/\n",
    "                func/\n",
    "                    preproc/\n",
    "                        # fmri preprocessing  derivatives\n",
    "                    registered/\n",
    "                        # fmri registration derivatives\n",
    "                    cleaned/\n",
    "                        # fmri nuisance-corrected derivatives\n",
    "                    timeseries/\n",
    "                        parcellation/\n",
    "                            # fmri timeseries\n",
    "                    connectomes/\n",
    "                        parcellation/\n",
    "                            # fmri connectomes\n",
    "                    qa/\n",
    "                        # fmri qa, for each from above\n",
    "                        ...\n",
    "                dwi/\n",
    "                    preproc/\n",
    "                        # dwi preprocessing  derivatives\n",
    "                    registered/\n",
    "                        # dwi registration derivatives\n",
    "                    tensor/\n",
    "                        # dwi tensor model\n",
    "                    fibers/\n",
    "                    connectomes/\n",
    "                        parcellation/\n",
    "                            # dwi connectomes\n",
    "                    qa/\n",
    "                        # dwi qa, for each from above\n",
    "                        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this schema gives rise to a hierarchical structure, we decided to use MongoDB to store the data. The data we are registering in particular are the CSV metadata files and the derivative url's that are present in http://m2g.io. Furthermore, the fact that our LIMS is an MVP and the schema could easily change also supported the fact that we should use MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Database\n",
    "Prequisites: all dependencies are present (PyMongo, MongoDB, scrapers). To install dependencies, just run \"pip3 install -r requirements.txt\" in the root directory of this git repository. To install MongoDB, follow the instructions here: https://docs.mongodb.com/manual/installation/.\n",
    "\n",
    "### 1. Initialize MongoDB Instance\n",
    "After all dependencies are built, make sure you have a MongoDB Instance running on your localhost on port 27017. To do this, just create a data directory and run \"mongod --dbpath path/to/datadir/\" in Terminal.\n",
    "\n",
    "<img src=\"./images/mongod_eg.png\" alt=\"Example\" style=\"width: 60%;\"/>\n",
    "\n",
    "### 2. Run (python3) database.py in the modules [directory](https://github.com/rguo123/m2g-lims/tree/master/modules)\n",
    "The code in the directory should automatically scrape m2g.io urls, csv files, and store them correctly in a MongoDB collection. Then you are done! The rest of this notebook will explain the steps in more detail and end with current issues we are having.\n",
    "\n",
    "\n",
    "## Initializing Database and Collection\n",
    "In database.py, the first thing we do is specify the actual database and collection we will be inserting our data into. All of this occurs in the \"init_database\" function. We name our database \"m2gdatabase\" and the collection \"lims\". This information should not be necessary unless you are a developer and want to explore the acutal database using Mongo or connect to the database for another application.\n",
    "\n",
    "Note that a Mongo Instance must be running locally on port 27017!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_database():\n",
    "    try:\n",
    "        client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "        # specify database\n",
    "        db = client.m2gdatabase\n",
    "        # specify collection\n",
    "        lims = db.lims\n",
    "        return lims\n",
    "    except:\n",
    "        raise Exception(\"MongoDB not running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Database Code\n",
    "The main function that builds our database is (surprise) the build_database() function. This is further modularized into other build methods that we will mention later. Below is the overall function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_database():\n",
    "    scrape = scraper.m2g_scrape('https://m2g.io')\n",
    "    data = scraper.m2g_data_scrape(scrape)\n",
    "    data = scraper.dive_deeper(data)\n",
    "    data['FMRI']['NKI1']['Preproc Images'] = data['FMRI']['NKI1'].pop('Preproc. Images')\n",
    "    lims = init_database()\n",
    "\n",
    "    for datatype in data.keys():\n",
    "        for dataset in data[datatype].keys():\n",
    "            build_dataset(lims, dataset)\n",
    "            for derivative in data[datatype][dataset].keys():\n",
    "                ## ignore graph QA plots since not subject based\n",
    "                ## TODO: think of fix for this\n",
    "                if (derivative == \"QA.graphs\"):\n",
    "                    continue\n",
    "                links = data[datatype][dataset][derivative]\n",
    "                build_derivative(lims, dataset, datatype, derivative, links)\n",
    "\n",
    "    build_metadata(lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 5 lines are what trigger the actual web scraper code that will parse the urls from m2g.io. Read the Web_Scraper notebook to learn more. Some really important preprocessing we do here is get rid of all periods in the original key names. This is because MongoDB will automatically parse a period into a nested document (i.e. hello.word becomes hello{world{}}). We use this nested functionality a lot for the Graphs and QA sections in m2g.io in order to preserve the already present subdirectories ('Graphs' is followed by sub directories of graphs). Luckily, the only one we found was NKI1's Preproccesed Images key which we just rename.\n",
    "\n",
    "After we have scraped all the keys, we insert a dataset document each time we encounter a specific url. This occurs in the build_dataset() function. We only build this dataset if it does not exist.\n",
    "\n",
    "The rest of the calls just correctly builds each derivative and organized that derivative by subject and datatype. The function ends with a call to build metadata for each subject.\n",
    "\n",
    "**NOTE**:\n",
    "Another important thing we do is remove quality assurance for Graphs. This is because these QA figures are not easily organized by subject and setting up a different dictionary for these figures would require a different scraper. This is definitely on our TODO list.\n",
    "\n",
    "### MongoDB Database Schema\n",
    "Before we continue, a figure of how the data actually looks in our MongoDB is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SCHEMA:\n",
    "Dataset:\n",
    "{\n",
    "    _id = dataset_name\n",
    "    _subjects = [list of subject document ids]\n",
    "}\n",
    "\n",
    "Subject:\n",
    "{\n",
    "    _id = subject id\n",
    "    metadata = {\n",
    "        session: {\n",
    "            {key: value},\n",
    "            {key: value},\n",
    "        }\n",
    "        ...\n",
    "    }\n",
    "    datatype = {\n",
    "        derivative: [\n",
    "            {link : neuroglancer link},\n",
    "            {link : neuroglancer link},\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are essentially storing two main documents: dataset and subject. As mentioned already, the build_dataset() function correctly inserts a dataset document in the format specified above. Note by keeping a list of subjects that we can update, we are fullfilling the dataset hierarchy level in our Model Schema.\n",
    "\n",
    "For the subject document, we organize everything as we specified in the decided upon schema. We do insert the actual derivative URL's as a dictionary though where the key is the actual URL and the value is a neuroglancer link. We have not implemented the visualization neuroglancer links yet. However, we have discussed with Jovo that this is something that would be great to have at some point so we are including it for the future.\n",
    "\n",
    "### Build Derivatives\n",
    "The build_derivatives() function handles the bulk of inserting everything and organizing them by subject. In this function, we handle adding a URL to it's respective derivative (Graph, Aligned Image, QA, etc) and datatype (fRMI, DWI). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_derivative(lims, dataset, datatype, derivative, links):\n",
    "\n",
    "    global scan_count\n",
    "    for link_list in links:\n",
    "        scan_count += 1\n",
    "        link_header = link_list[0]\n",
    "        ## invalid parse\n",
    "        ## TODO: get better fix\n",
    "        if (link_header.find(\"sub\") < 0 or link_header.find(\"_\") < 0):\n",
    "            continue\n",
    "        url = encode_url(link_list[1])\n",
    "\n",
    "        subject = get_subject(link_header)\n",
    "        update_dataset(lims, dataset, subject)\n",
    "        # insert  and subject dataype if needed\n",
    "\n",
    "        write_result = lims.update_one(\n",
    "            filter = {\"_id\": subject},\n",
    "            update = {\"$setOnInsert\": { datatype + \".\" + derivative: [{url: \"\"}]}},\n",
    "            upsert = True\n",
    "        )\n",
    "\n",
    "        if (write_result.upserted_id is not None):\n",
    "            print(\"Upsert and added Scan #\" + str(scan_count))\n",
    "            continue\n",
    "\n",
    "        lims.update_one(\n",
    "            filter = {\"_id\": subject, datatype + \".\" + derivative: {\"$exists\": False}},\n",
    "            update = { \"$set\": { datatype + \".\" + derivative: [] }},\n",
    "        )\n",
    "        #insert url to derivative list\n",
    "        #NOTE: no upsert option exists\n",
    "        lims.update_one(\n",
    "            filter = {\"_id\": subject}, #query\n",
    "            update = {\n",
    "                \"$push\": { datatype + \".\" + derivative: {url: \"\"} }\n",
    "            }\n",
    "        )\n",
    "        print(\"Added Scan #\" + str(scan_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difficult part in writing this function is to know when to create a new document and when to just update. We do not want to insert multiple subject documents as this would throw a MongoDB error. This is also important for specifying the datatype dictionary the derivative is going into. We should only make each datatype field once! Finally, since each derivative can have multiple URL's, we must know when to create a derivative list or when to just push a url to an existing derivative list.\n",
    "\n",
    "Thus the main bulk of the function start with seeing if the subject exists in the database. If it doesn't, we can add in everything right away including inserting a correct dataset dictionary and derivative list (don't need to worry about losing info since it's the first occurrence of that subject). We can immediately move on to the next url if this is the case.\n",
    "\n",
    "If it isn't, then we need to worry about whether we should create a new derivative list or just push a link to an existing one. That's why we update with the filter that the derivative list does not exist first (nothing will happen if the list does exist). After, we can push a new url without worrying about anything. As mentioned already, we store the url as a dictionary for the neuroglancer link. The value is just an empty string until we actually do this part.\n",
    "\n",
    "Lastly, in storing the actual URL we have to remove all the periods (see reason above). We just encode each period as \"$$$\" and make sure to decode it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Metadata\n",
    "Building metadata is a lot simpler than building derivatives since we just store everything as key:value pairs in a metadata dictionary. Each key is a metadata field and each value is well the value. Similar to building the derivative urls, we first scrape all the CSV's and then update the database.\n",
    "\n",
    "Since each subject can have multiple of the same metadata field if the subject has more than one scan, we just group the metadata's together by scan. We can tell which scan is which by the \"SESSION\" metadata field. If this field does not exist, we just provide a default as \"Session-1\".\n",
    "\n",
    "**Really Important Quirks**:\n",
    "To find the correct subject to add the metadata to, we need a subject id. Luckily, this is given for us in the CSV. However, extremely unlucky for us, each CSV has a different name for the subject ID (URSI, SubjectID, SUBID). Right now, we just brute force all options that we found since there are relatively few CSV files. \n",
    "\n",
    "Another really important thing is that subject ID's sometimes have 2 zeroes in front (e.g. 0025864). However, CSV files just drop those zeros so then we have incorrect id's. Right now, we just try to add in two zeroes to an ID if that subject does not exist in the database. If it still doesn't work, we just do not add in that particular piece of metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_metadata(lims):\n",
    "    ## Get metadata CSV\n",
    "    try:\n",
    "        links = csvs.get_csv_links(SOURCE_URL)\n",
    "        filenames = csvs.download_csvs(links, DATA_PATH)\n",
    "        for filename in filenames:\n",
    "            metadata_list = csvs.parse_csv(filenames)\n",
    "    except:\n",
    "        raise Exception(\"Could not access/download CSVs\")\n",
    "\n",
    "    for metadata in metadata_list:\n",
    "\n",
    "        # Try to get subject ID, no other way besides brute force right now\n",
    "        subid = metadata.pop(\"SUBID\", -1)\n",
    "        if (subid == -1):\n",
    "            subid = metadata.pop(\"URSI\", -1)\n",
    "        if (subid == -1):\n",
    "            subid = metadata.pop(\"ursi\", -1)\n",
    "        if (subid == -1):\n",
    "            subid = metadata.pop(\"SubjectID\", -1)\n",
    "        if (subid == -1):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            session = metadata.pop(\"SESSION\")\n",
    "        except:\n",
    "            session = \"Session-1\"\n",
    "\n",
    "        for key, value in metadata.items():\n",
    "            try:\n",
    "                result = lims.update_one(\n",
    "                    filter = {\"_id\": \"sub-\" + subid},\n",
    "                    update = {\"$set\": {\"metadata.\" + session + \".\" + key: value}}\n",
    "                )\n",
    "\n",
    "                ##Update did not occur\n",
    "                if (result.matched_count < 1):\n",
    "                    lims.update_one(\n",
    "                        filter = {\"_id\": \"sub-00\" + subid},\n",
    "                        update = {\"$set\": {\"metadata.\" + session + \".\" + key: value}}\n",
    "                    )\n",
    "            except:\n",
    "                raise Exception(\"Error adding metadata\")\n",
    "\n",
    "    print(\"Added Metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's everything!\n",
    "Below is a final snapshot of what all this looks like in the raw database. (Disclaimer: it's looks pretty bad :()\n",
    "<img src=\"./images/raw_db.png\" alt=\"raw_db\" style=\"width: 80%;\"/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
