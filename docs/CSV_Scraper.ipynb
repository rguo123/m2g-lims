{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Scraper\n",
    "The process for this is almost identical to [web scraping](https://nbviewer.jupyter.org/github/rguo123/m2g-lims/blob/master/docs/Web_Scraper.ipynb). The only difference is we have to download all the links in order to parse the CSVs. \n",
    "\n",
    "This is very easy in Python. We can just call urllib.requests.urlretrieve() which is a built in library for Python 3. However, what's important to note is that our code needs a data_path to store the CSVs. In the github repository, we provide one for you in [data/csv/](https://github.com/rguo123/m2g-lims/tree/master/data/csv). If you want to store the CSVs somewhere else, you must edit the code in csv_scraper.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_csvs(csv_links, data_path):\n",
    "    filenames = []\n",
    "    for link in csv_links:\n",
    "        filename = data_path + link.split('/')[-1]\n",
    "        filenames.append(filename)\n",
    "        urllib.request.urlretrieve(link, filename)\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing CSVs\n",
    "The only thing left to mention is how we parse the CSV data. It's all very straightforward and is completely dependent on another built-in Python Library: csv. However, we would like to mention that we toss out all metadata that does not have a value, which is denoted by a \"#\" in the csv file. This is just so we reduce clutter in our LIMS. However, this does result in uneven metadata among subjects in the same dataset and even for session scans for the same subject.  \n",
    "\n",
    "We return the parsed data as a list of dictionaries where each dictionary is the metadata for a particular scan session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_csv(filenames):\n",
    "    metadata_list = []\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            # get metadata fields\n",
    "            keys = next(reader)\n",
    "            # remaining lines are subject metadata\n",
    "            for row in reader:\n",
    "                metadata = {}\n",
    "                for i in range(len(row)):\n",
    "                    ## skip if empty\n",
    "                    if row[i] == \"#\":\n",
    "                        continue\n",
    "                    metadata[keys[i]] = row[i]\n",
    "                metadata_list.append(metadata)\n",
    "\n",
    "    return metadata_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
